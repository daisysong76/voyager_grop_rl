import copy
import json
import os
import time
from typing import Dict

import voyager.utils as U
from .env import VoyagerEnv

from .agents import ActionAgent
from .agents import CriticAgent
from .agents import CurriculumAgent
from .agents import SkillManager
from .agents import VisionAgent

# TODO 1: should I add it to skillmanager?
# from utils.graph_rag_manager import GraphRAGManager

# TODO 1: Add Graph RAG for Retrieval-Augmented Reasoning
#from utils.scene_graph import SceneGraph
# TODO 1: Initialize the scene graph
#scene_graph = SceneGraph()

# TODO 2: 
# Main control file that integrates the DSPy agent
#from dspy import DSPyPromptOptimizer  # Import DSPy
#from dspy import Program, Signature, Value  # Import necessary DSPy components
#from utils.prompt_optimizer_dspy import VoyagerPromptOptimizer

# TODO orginal: remove event memory
class Voyager:
    def __init__(
        self,
        #bot_username: str = "bot",
        bot_id=0,
        mc_port: int = None,
        azure_login: Dict[str, str] = None,
        server_port: int = 3000,
        viewer_port: int = 3001,  # Add default viewer port
        username: str = None,      # Add default username
        openai_api_key: str = None,
        #ANTHROPIC_API_KEY: str = None,  # Updated to Claude API key
        env_wait_ticks: int = 20,
        env_request_timeout: int = 600,
        pause_on_think: bool = True,
        max_iterations: int = 160,
        reset_placed_if_failed: bool = False,
     
        vision_agent_model_name: str = "gpt-4-turbo",
        vision_agent_temperature: float = 0,
        # TODO 2: add vision agent qa model name
        vision_agent_qa_model_name: str = "gpt-4",
        vision_agent_qa_temperature: float = 0,

        action_agent_model_name: str = "gpt-4",
        action_agent_temperature: float = 0,
        action_agent_task_max_retries: int = 4,
        action_agent_show_chat_log: bool = True,
        action_agent_show_execution_error: bool = True,
        curriculum_agent_model_name: str = "gpt-4",
        curriculum_agent_temperature: float = 0,
        curriculum_agent_qa_model_name: str = "gpt-4",
        curriculum_agent_qa_temperature: float = 0,
        curriculum_agent_warm_up: Dict[str, int] = None,
        curriculum_agent_core_inventory_items: str = r".*_log|.*_planks|stick|crafting_table|furnace"
        r"|cobblestone|dirt|coal|.*_pickaxe|.*_sword|.*_axe",
        curriculum_agent_mode: str = "auto",
        critic_agent_model_name: str = "gpt-4",
        critic_agent_temperature: float = 0,
        critic_agent_mode: str = "auto",
        skill_manager_model_name: str = "gpt-4",
        skill_manager_temperature: float = 0,
        skill_manager_retrieval_top_k: int = 5,
        #claude_api_request_timeout: int = 240,  # Updated parameter name
        openai_api_request_timeout: int = 240,
        ckpt_dir: str = "ckpt",
        skill_library_dir: str = None,
        resume: bool = False,
        # TODO 1: Add Graph RAG for Retrieval-Augmented Reasoning
        # scene_graph, 
        # vectordb,
    ):
        """
        The main class for Voyager.
        Action agent is the iterative prompting mechanism in paper.
        Curriculum agent is the automatic curriculum in paper.
        Critic agent is the self-verification in paper.
        Skill manager is the skill library in paper.
        :param mc_port: minecraft in-game port
        :param azure_login: minecraft login config
        :param server_port: mineflayer port
        :param openai_api_key: openai api key
        :param env_wait_ticks: how many ticks at the end each step will wait, if you found some chat log missing,
        you should increase this value
        :param env_request_timeout: how many seconds to wait for each step, if the code execution exceeds this time,
        python side will terminate the connection and need to be resumed
        :param reset_placed_if_failed: whether to reset placed blocks if failed, useful for building task
        :param action_agent_model_name: action agent model name
        :param action_agent_temperature: action agent temperature
        :param action_agent_task_max_retries: how many times to retry if failed
        :param curriculum_agent_model_name: curriculum agent model name
        :param curriculum_agent_temperature: curriculum agent temperature
        :param curriculum_agent_qa_model_name: curriculum agent qa model name
        :param curriculum_agent_qa_temperature: curriculum agent qa temperature
        :param curriculum_agent_warm_up: info will show in curriculum human message
        if completed task larger than the value in dict, available keys are:
        {
            "context": int,
            "biome": int,
            "time": int,
            "other_blocks": int,
            "nearby_entities": int,
            "health": int,
            "hunger": int,
            "position": int,
            "equipment": int,
            "chests": int,
            "optional_inventory_items": int,
        }
        :param curriculum_agent_core_inventory_items: only show these items in inventory before optional_inventory_items
        reached in warm up
        :param curriculum_agent_mode: "auto" for automatic curriculum, "manual" for human curriculum
        :param critic_agent_model_name: critic agent model name
        :param critic_agent_temperature: critic agent temperature
        :param critic_agent_mode: "auto" for automatic critic ,"manual" for human critic
        :param skill_manager_model_name: skill manager model name
        :param skill_manager_temperature: skill manager temperature
        :param skill_manager_retrieval_top_k: how many skills to retrieve for each task
        :param claude_api_request_timeout: how many seconds to wait for openai api
        :param ckpt_dir: checkpoint dir
        :param skill_library_dir: skill library dir
        :param resume: whether to resume from checkpoint
        """
        # init env
        self.bot_id = bot_id

        # Calculate unique ports based on bot_id
        server_port = 3000 + (bot_id * 2)          # Mineflayer port
        viewer_port = 3001 + (bot_id * 2)          # Prismarine viewer port

        # Assign unique username
        username = username or f"bot_{bot_id}"

        # Modify checkpoint dir to be unique per bot
        ckpt_dir = os.path.join(ckpt_dir, f"bot_{bot_id}")
        if not os.path.exists(ckpt_dir):
            os.makedirs(ckpt_dir)

        # Initialize environment with unique ports and username
        self.env = VoyagerEnv(
            mc_port=mc_port,
            azure_login=azure_login,
            server_port=server_port,
            viewer_port=viewer_port,
            username=username,
            request_timeout=env_request_timeout,
            pause_on_think=pause_on_think,
        )
        self.env_wait_ticks = env_wait_ticks
        self.reset_placed_if_failed = reset_placed_if_failed
        self.max_iterations = max_iterations

        #TODO 1: Add Graph RAG for Retrieval-Augmented Reasoning
        # self.scene_graph = scene_graph
        # self.graph_rag_manager = GraphRAGManager(scene_graph, vectordb)
        # self.instruction_learner = InstructionLearning()

        # set openai api key
        os.environ["OPENAI_API_KEY"] = openai_api_key
        #TODO change to claude api key
        #os.environ["ANTHROPIC_API_KEY"] = ANTHROPIC_API_KEY

        # init agents
        # Receive the task and context from the Curriculum Agent.
        # Consult the Vision Agent for visual data analysis relevant to the task.
        # Plan the actions required to complete the task based on both the curriculum and visual insights.
        # Execute the actions in the environment and collect results.
        self.vision_agent = VisionAgent(
            model_name=vision_agent_model_name,
            temperature=vision_agent_temperature,
            request_timeout=openai_api_request_timeout,
            ckpt_dir=ckpt_dir,
            resume=resume,
        )
        self.vision_agent_rollout_num_iter = -1

        self.action_agent = ActionAgent(
            # action agent is the iterative prompting mechanism in paper
            # TODO 1: Add Graph RAG for Retrieval-Augmented Reasoning
            model_name=action_agent_model_name,
            temperature=action_agent_temperature,
            request_timout=openai_api_request_timeout,
            ckpt_dir=ckpt_dir,
            resume=resume,
            chat_log=action_agent_show_chat_log,
            execution_error=action_agent_show_execution_error,
            vision_agent=self.vision_agent,
        )
        self.action_agent_task_max_retries = action_agent_task_max_retries
        
        # Role: Proposes tasks based on the current state of the environment and the agent's learning progress.
        # Analyze the current context and previous tasks completed.
        # Propose the next task to the Action Agent, which may include visual tasks that require input from the Vision Agent.
        # Provide context and any necessary instructions for the task.
        self.curriculum_agent = CurriculumAgent(
            model_name=curriculum_agent_model_name,
            temperature=curriculum_agent_temperature,
            qa_model_name=curriculum_agent_qa_model_name,
            qa_temperature=curriculum_agent_qa_temperature,
            request_timout=openai_api_request_timeout,
            ckpt_dir=ckpt_dir,
            resume=resume,
            mode=curriculum_agent_mode,
            warm_up=curriculum_agent_warm_up,
            core_inventory_items=curriculum_agent_core_inventory_items,
            # TODO 2: how to debug if this following vision_agent was called?
            vision_agent=self.vision_agent,
        )

        # Actions:
        # Receive the results from the Action Agent, including any visual data processed.
        # Assess the success of the task based on predefined criteria, including visual accuracy.
        # Provide feedback to the Action Agent regarding its performance, particularly in relation to visual tasks.

        self.critic_agent = CriticAgent(
            model_name=critic_agent_model_name,
            temperature=critic_agent_temperature,
            request_timout=openai_api_request_timeout,
            mode=critic_agent_mode,
            vision_agent=self.vision_agent,
        )

        self.skill_manager = SkillManager(
            # TODO 1: Add Graph RAG for Retrieval-Augmented Reasoning
            model_name=skill_manager_model_name,
            temperature=skill_manager_temperature,
            retrieval_top_k=skill_manager_retrieval_top_k,
            request_timout=openai_api_request_timeout,
            ckpt_dir=skill_library_dir if skill_library_dir else ckpt_dir,
            resume=True if resume or skill_library_dir else False,
            vision_agent=self.vision_agent,
        )
        self.recorder = U.EventRecorder(ckpt_dir=ckpt_dir, resume=resume)
        self.resume = resume

        # init variables for rollout
        self.action_agent_rollout_num_iter = -1
        self.task = None
        self.context = ""
        self.messages = None
        self.conversations = []
        self.last_events = None

    # TODO 1: Add Graph RAG for Retrieval-Augmented Reasoning
    # def execute_task(self, instruction):
    #     # Step 1: Parse instruction
    #     parsed_instruction = self.instruction_learner.parse_instruction(instruction)
    #     # Step 2: Retrieve context with Graph RAG
    #     context_docs = self.graph_rag_manager.retrieve_with_graph(parsed_instruction)
    #     # Step 3: Use the context and parsed instruction to inform decision-making
    #     result = self.make_decision(parsed_instruction, context_docs)
    #     return result
    
    def reset(self, task, context="", reset_env=True):
        self.action_agent_rollout_num_iter = 0
        self.task = task
        self.context = context
        if reset_env:
            self.env.reset(
                options={
                    "mode": "soft",
                    "wait_ticks": self.env_wait_ticks,
                }
            )
        difficulty = (
            "easy" if len(self.curriculum_agent.completed_tasks) > 15 else "peaceful"
        )
        # step to peek an observation
        events = self.env.step(
            "bot.chat(`/time set ${getNextTime()}`);\n"
            + f"bot.chat('/difficulty {difficulty}');"
        )
        skills = self.skill_manager.retrieve_skills(query=self.context)
        print(
            f"\033[33mRender Action Agent system message with {len(skills)} skills\033[0m"
        )
        system_message = self.action_agent.render_system_message(skills=skills)
        human_message = self.action_agent.render_human_message(
            events=events, code="", task=self.task, context=context, critique=""
        )
        self.messages = [system_message, human_message]
        print(
            f"\033[32m****Action Agent human message****\n{human_message.content}\033[0m"
        )
        assert len(self.messages) == 2
        self.conversations = []
        return self.messages

    def close(self):
        self.env.close()

    def step(self):
        if self.action_agent_rollout_num_iter < 0:
            raise ValueError("Agent must be reset before stepping")
        # old: ai_message = self.action_agent.llm(self.messages)
        ai_message = self.action_agent.llm.invoke(self.messages)
        print(f"\033[34m****Action Agent ai message****\n{ai_message.content}\033[0m")
        self.conversations.append(
            (self.messages[0].content, self.messages[1].content, ai_message.content)
        )
        parsed_result = self.action_agent.process_ai_message(message=ai_message)
        success = False
        if isinstance(parsed_result, dict):
            code = parsed_result["program_code"] + "\n" + parsed_result["exec_code"]
            events = self.env.step(
                code,
                programs=self.skill_manager.programs,
            )
            self.recorder.record(events, self.task)
            self.action_agent.update_chest_memory(events[-1][1]["nearbyChests"])
            success, critique = self.critic_agent.check_task_success(
                events=events,
                task=self.task,
                context=self.context,
                chest_observation=self.action_agent.render_chest_observation(),
                max_retries=5,
            )

            if self.reset_placed_if_failed and not success:
                # revert all the placing event in the last step
                blocks = []
                positions = []
                for event_type, event in events:
                    if event_type == "onSave" and event["onSave"].endswith("_placed"):
                        block = event["onSave"].split("_placed")[0]
                        position = event["status"]["position"]
                        blocks.append(block)
                        positions.append(position)
                new_events = self.env.step(
                    f"await givePlacedItemBack(bot, {U.json_dumps(blocks)}, {U.json_dumps(positions)})",
                    programs=self.skill_manager.programs,
                )
                events[-1][1]["inventory"] = new_events[-1][1]["inventory"]
                events[-1][1]["voxels"] = new_events[-1][1]["voxels"]
            new_skills = self.skill_manager.retrieve_skills(
                query=self.context
                + "\n\n"
                + self.action_agent.summarize_chatlog(events)
            )
            system_message = self.action_agent.render_system_message(skills=new_skills)
            human_message = self.action_agent.render_human_message(
                events=events,
                code=parsed_result["program_code"],
                task=self.task,
                context=self.context,
                critique=critique,
            )
            self.last_events = copy.deepcopy(events)
            self.messages = [system_message, human_message]
        else:
            assert isinstance(parsed_result, str)
            self.recorder.record([], self.task)
            print(f"\033[34m{parsed_result} Trying again!\033[0m")
        assert len(self.messages) == 2
        self.action_agent_rollout_num_iter += 1
        done = (
            self.action_agent_rollout_num_iter >= self.action_agent_task_max_retries
            or success
        )
        info = {
            "task": self.task,
            "success": success,
            "conversations": self.conversations,
        }
        if success:
            assert (
                "program_code" in parsed_result and "program_name" in parsed_result
            ), "program and program_name must be returned when success"
            info["program_code"] = parsed_result["program_code"]
            info["program_name"] = parsed_result["program_name"]
        else:
            print(
                f"\033[32m****Action Agent human message****\n{self.messages[-1].content}\033[0m"
            )
        return self.messages, 0, done, info

    def rollout(self, *, task, context, reset_env=True):
        # TODO 2: Add a vision agent to the rollout
        # Capture image for analysis at the start of the rollout
        # image_data 
        # vision_analysis = self.vision_agent.analyze_image(image_data)

        self.reset(task=task, context=context, reset_env=reset_env)
        while True:
            messages, reward, done, info = self.step()
            if done:
                break
        return messages, reward, done, info

    def learn(self, reset_env=True):
        if self.resume:
            # keep the inventory
            self.env.reset(
                options={
                    "mode": "soft",
                    "wait_ticks": self.env_wait_ticks,
                }
            )
        else:
            # clear the inventory
            self.env.reset(
                options={
                    "mode": "hard",
                    "wait_ticks": self.env_wait_ticks,
                }
            )
            self.resume = True
        self.last_events = self.env.step("")

        while True:
            if self.recorder.iteration > self.max_iterations:
                print("Iteration limit reached")
                break
            task, context = self.curriculum_agent.propose_next_task(
                events=self.last_events,
                chest_observation=self.action_agent.render_chest_observation(),
                max_retries=5,
            )
            print(
                f"\033[35mStarting task {task} for at most {self.action_agent_task_max_retries} times\033[0m"
            )
            try:
                messages, reward, done, info = self.rollout(
                    task=task,
                    context=context,
                    reset_env=reset_env,
                )
            except Exception as e:
                time.sleep(3)  # wait for mineflayer to exit
                info = {
                    "task": task,
                    "success": False,
                }
                # reset bot status here
                self.last_events = self.env.reset(
                    options={
                        "mode": "hard",
                        "wait_ticks": self.env_wait_ticks,
                        "inventory": self.last_events[-1][1]["inventory"],
                        "equipment": self.last_events[-1][1]["status"]["equipment"],
                        "position": self.last_events[-1][1]["status"]["position"],
                    }
                )
                # use red color background to print the error
                print("Your last round rollout terminated due to error:")
                print(f"\033[41m{e}\033[0m")

            if info["success"]:
                self.skill_manager.add_new_skill(info)

            self.curriculum_agent.update_exploration_progress(info)
            print(
                f"\033[35mCompleted tasks: {', '.join(self.curriculum_agent.completed_tasks)}\033[0m"
            )
            print(
                f"\033[35mFailed tasks: {', '.join(self.curriculum_agent.failed_tasks)}\033[0m"
            )

        return {
            "completed_tasks": self.curriculum_agent.completed_tasks,
            "failed_tasks": self.curriculum_agent.failed_tasks,
            "skills": self.skill_manager.skills,
        }

    def decompose_task(self, task):
        if not self.last_events:
            self.last_events = self.env.reset(
                options={
                    "mode": "hard",
                    "wait_ticks": self.env_wait_ticks,
                }
            )
        return self.curriculum_agent.decompose_task(task, self.last_events)

    def inference(self, task=None, sub_goals=[], reset_mode="hard", reset_env=True):
        if not task and not sub_goals:
            raise ValueError("Either task or sub_goals must be provided")
        if not sub_goals:
            sub_goals = self.decompose_task(task)
        self.env.reset(
            options={
                "mode": reset_mode,
                "wait_ticks": self.env_wait_ticks,
            }
        )
        self.curriculum_agent.completed_tasks = []
        self.curriculum_agent.failed_tasks = []
        self.last_events = self.env.step("")
        while self.curriculum_agent.progress < len(sub_goals):
            next_task = sub_goals[self.curriculum_agent.progress]
            context = self.curriculum_agent.get_task_context(next_task)
            print(
                f"\033[35mStarting task {next_task} for at most {self.action_agent_task_max_retries} times\033[0m"
            )
            messages, reward, done, info = self.rollout(
                task=next_task,
                context=context,
                reset_env=reset_env,
            )
            self.curriculum_agent.update_exploration_progress(info)
            print(
                f"\033[35mCompleted tasks: {', '.join(self.curriculum_agent.completed_tasks)}\033[0m"
            )
            print(
                f"\033[35mFailed tasks: {', '.join(self.curriculum_agent.failed_tasks)}\033[0m"
            )

   
    # TODO 3: add more functions to interact with the environment and communicate with the agent
    # def send_message(self, message):
    #     self.env.step(f"bot.chat(`{message}`);\n")

    # def receive_message(self, message):
    #     self.env.step(f"bot.chat(`{message}`);\n")

    # def process_message(self, message):
    #     self.env.step(f"bot.chat(`{message}`);\n")
    
    # def cooperate(self, message):
    #     # Example task cooperation
    #     self.send_message("I need data for task X.", other_bot)
    #     # Wait for a response and process it
    #     # Implement logic to handle the response and complete the task
    #     self.env.step(f"bot.chat(`{message}`);\n")

    # # def get_inventory(self):
    # #     return self.env.step("bot.getInventory();\n")

    # # def get_equipment(self):
    # #     return self.env.step("bot.getEquipment();\n")
    # def move_to(self, position):
    #     self.env.step(f"bot.moveTo({position});\n")

    # def get_position(self):
    #     return self.env.step("bot.getPosition();\n")
   

# 1. Collaborative Learning
# Description: This approach allows multiple agents to work together, share insights, and critique one another, leading to solutions that exceed what any single agent could achieve in isolation. Collaborative learning enhances problem-solving capabilities by enabling agents to debate ideas and split tasks effectively.
# Cutting-Edge Aspects:
# Integration with Large Language Models (LLMs): The use of LLMs in conjunction with MAS can significantly improve the quality of results, as agents can leverage the advanced capabilities of LLMs for understanding context and semantics.
# Dynamic Adaptation: Agents can adapt their strategies based on real-time feedback from their peers, leading to more robust and flexible solutions in complex environments.
# Applications: This approach is particularly promising in fields like automated negotiation in e-commerce, swarm robotics, and environmental monitoring, where agents can learn from each other and improve their collective performance.
# 2. Advanced Reasoning, Planning, and Problem-Solving
# Description: Equipping agents with higher-level cognitive skills allows them to break down complex problems and adapt to changing circumstances. This approach enhances the agents' ability to handle multifaceted tasks and improves their adaptability in dynamic environments.
# Cutting-Edge Aspects:
# Hierarchical Planning: Implementing hierarchical planning systems enables agents to manage complex tasks by breaking them down into smaller, manageable subtasks, which can be executed concurrently.
# Scenario Simulation: Using advanced simulation tools allows agents to plan for various contingencies, improving their ability to adapt to unexpected changes in the environment.
# Applications: This approach is particularly relevant in areas such as smart city management, disaster response, and complex supply chain optimization, where agents must navigate intricate scenarios and make informed decisions.
# Conclusion
# Both Collaborative Learning and Advanced Reasoning, Planning, and Problem-Solving represent cutting-edge approaches in the field of multi-agent systems. They leverage the latest advancements in AI and machine learning to enhance the capabilities of autonomous agents, enabling them to tackle complex, real-world challenges more effectively. As these technologies continue to evolve, they will likely play a pivotal role in shaping the future of intelligent systems across various industries.
#    1. Collaborative Learning
# Description: Allowing agents to work together, critique one another, and share insights can lead to solutions that exceed what any single agent could achieve in isolation. This approach fosters a more comprehensive understanding of complex problems and encourages creative solutions.
# Benefits: By leveraging the strengths of each agent, the system can explore a wider range of possibilities and uncover innovative approaches that individual agents might overlook. This collaborative dynamic can significantly enhance the overall capabilities of the multi-agent system.
# Implementation: For example, a framework where a more advanced agent (like a GPT-4) mentors less advanced agents (like GPT-3.5) can accelerate learning and improve task execution efficiency.
# 2. Dynamic Task Allocation
# Description: Implementing adaptive task assignment allows the Curriculum Agent to dynamically allocate tasks based on the current workload and performance of each agent. This flexibility can help balance the workload and optimize resource utilization.
# Benefits: By ensuring that tasks are assigned to the most suitable agents based on their current capabilities and context, the system can improve efficiency and effectiveness in task execution. This approach also allows for better handling of unexpected changes in the environment or agent performance.
# Implementation: For instance, if one agent is overloaded or underperforming, the system can reassign tasks to other agents that are better equipped to handle them at that moment.
# 3. Advanced Reasoning, Planning, and Problem-Solving
# Description: Equipping agents with higher-level cognitive skills, such as the ability to break down complex problems and adapt to changing circumstances, can expand the range and sophistication of tasks they can tackle.
# Benefits: This approach enhances the agents' ability to handle multifaceted tasks and improves their adaptability in dynamic environments. Techniques like chain-of-thought prompting and multi-agent debate can facilitate deeper reasoning and more effective problem-solving.
# Implementation: By integrating advanced reasoning capabilities, agents can better understand the context of their tasks, anticipate challenges, and devise more effective strategies for task completion.

# 1. Enhanced Communication Protocols
# Implement Real-Time Messaging: Use a robust messaging system (like WebSockets or message queues) to facilitate real-time communication between agents. This allows for quicker updates and responses.
# Standardize Message Formats: Define clear message formats and protocols for communication to ensure that all agents understand the information being exchanged.
# 2. Dynamic Task Allocation
# Adaptive Task Assignment: Allow the Curriculum Agent to dynamically assign tasks based on the current workload and performance of each agent. This can help balance the workload and optimize resource utilization.
# Agent Specialization: Encourage agents to specialize in certain tasks based on their strengths and past performance, allowing for more efficient task execution.
# 3. Feedback Loops and Continuous Learning
# Implement Continuous Feedback Mechanisms: Create a system where agents can provide feedback to each other after task completion. This can help improve future performance and decision-making.
# Utilize Reinforcement Learning: Incorporate reinforcement learning techniques to allow agents to learn from their successes and failures over time, adapting their strategies accordingly.
# 4. Reflection and Self-Assessment
# Integrate Reflection Mechanisms: Allow agents to periodically review their own performance and outputs. This can help them identify areas for improvement and refine their strategies.
# Peer Review System: Implement a peer review system where agents can evaluate each other's outputs, providing constructive feedback and suggestions for improvement.
# 5. Utilize Advanced Planning Techniques
# Hierarchical Planning: Implement a hierarchical planning system where complex tasks are broken down into smaller subtasks, allowing agents to focus on manageable components.
# Scenario Simulation: Use scenario simulation tools to allow agents to plan for various contingencies, improving their ability to adapt to unexpected changes in the environment.
# 6. Incorporate Predictive Analytics
# Data-Driven Decision Making: Use predictive analytics to analyze historical data and forecast future outcomes. This can help agents make more informed decisions and anticipate potential challenges.
# Real-Time Data Integration: Ensure that agents have access to real-time data to inform their actions and decisions, enhancing their responsiveness to changing conditions.
# 7. Improve Skill Management
# Skill Inventory System: Maintain a comprehensive inventory of skills and capabilities for each agent. This can help the Skill Manager identify gaps and recommend training or skill acquisition.
# Cross-Training Opportunities: Encourage cross-training among agents to enhance their versatility and ability to handle a wider range of tasks.
# 8. Implement Conflict Resolution Mechanisms
# Negotiation Protocols: Develop negotiation protocols for agents to resolve conflicts over resources or task assignments, ensuring smooth collaboration.
# Priority-Based Task Management: Establish a priority system for tasks to help agents determine which tasks to focus on first, reducing potential conflicts.
# 9. Monitor and Evaluate Performance
# Performance Metrics: Define clear performance metrics for each agent and regularly evaluate their effectiveness. Use these metrics to inform adjustments to the workflow.
# Dashboard for Monitoring: Create a centralized dashboard to monitor the status and performance of all agents in real-time, allowing for quick identification of issues.
# 10. Foster Collaboration and Teamwork
# Team-Based Objectives: Set team-based objectives that require collaboration among agents, encouraging them to work together towards common goals.
# Shared Knowledge Base: Develop a shared knowledge base where agents can access information, best practices, and lessons learned from previous tasks.

#     Workflow Overview
# Curriculum Agent:
# Role: Proposes tasks based on the current state of the environment and the agent's learning progress.
# Actions:
# Analyze the current context and previous tasks completed.
# Propose the next task to the Action Agent.
# Provide context and any necessary instructions for the task.
# Action Agent:
# Role: Executes the task proposed by the Curriculum Agent.
# Actions:
# Receive the task and context from the Curriculum Agent.
# Plan the actions required to complete the task.
# Execute the actions in the environment.
# Collect results and feedback from the environment.
# Critic Agent:
# Role: Evaluates the performance of the Action Agent after task execution.
# Actions:
# Receive the results from the Action Agent.
# Assess the success of the task based on predefined criteria (e.g., success metrics, quality of output).
# Provide feedback to the Action Agent regarding its performance.
# Suggest improvements or adjustments for future tasks.
# Skill Manager:
# Role: Manages the skills and capabilities of the Action Agent.
# Actions:
# Monitor the skills utilized in the Action Agent's actions.
# Update the skill library based on the skills used and their effectiveness.
# Provide feedback to the Action Agent on its skill usage.
# Facilitate learning and adaptation by suggesting new skills based on the tasks proposed by the Curriculum Agent.
# Detailed Workflow Steps
# Task Proposal:
# The Curriculum Agent analyzes the current state and proposes a new task to the Action Agent.
# Example: "Your next task is to gather resources from the specified location."
# 2. Task Execution:
# The Action Agent receives the task and context, plans the necessary actions, and executes them in the environment.
# Example: The Action Agent moves to the specified location and collects resources.
# 3. Performance Evaluation:
# After task execution, the Action Agent sends the results to the Critic Agent.
# Example: "I collected 10 resources from the location."
# Feedback and Improvement:
# The Critic Agent evaluates the results against success criteria and provides feedback.
# Example: "You successfully collected the resources, but you could improve your efficiency by using a different route next time."
# 5. Skill Management:
# The Skill Manager reviews the skills used during the task and updates the Action Agent's skill set based on the Critic's feedback.
# Example: "The Action Agent should learn a new skill for faster navigation."
# Iterative Learning:
# The Curriculum Agent proposes new tasks based on the updated skills and the overall learning progress of the agents.
# Example: "Now that you have improved your navigation skills, your next task is to explore a new area."


 # TODO 1: Add Graph RAG for Retrieval-Augmented Reasoning
    # def execute_task(self, instruction):
    #     # Step 1: Parse instruction
    #     parsed_instruction = self.instruction_learner.parse_instruction(instruction)
    #     # Step 2: Retrieve context with Graph RAG
    #     context_docs = self.graph_rag_manager.retrieve_with_graph(parsed_instruction)
    #     # Step 3: Use the context and parsed instruction to inform decision-making
    #     result = self.make_decision(parsed_instruction, context_docs)
    #     return result


#     Breakdown of the inference Function
# 1. Parameters:
# task: An optional parameter representing the main task to be executed.
# sub_goals: A list of sub-goals that need to be achieved as part of the task.
# reset_mode: A string indicating how the environment should be reset (e.g., "hard" or "soft").
# reset_env: A boolean indicating whether the environment should be reset before starting the inference process.
# 2. Initial Checks:
# The function first checks if either task or sub_goals is provided. If neither is provided, it raises a ValueError.
# If sub_goals is not provided, it calls the decompose_task method to generate sub-goals based on the main task.
# 3. Environment Reset:
# The environment is reset using the specified reset_mode and env_wait_ticks. This prepares the environment for the new task execution.
# Task Tracking:
# The function initializes lists to track completed and failed tasks by the curriculum_agent.
# 5. Execution Loop:
# The function enters a loop that continues until all sub-goals are completed. Within this loop:
# It retrieves the next sub-goal and its context.
# It prints a message indicating the start of the task.
# It calls the rollout method to execute the current sub-goal, passing the task and context.
# It updates the exploration progress of the curriculum_agent based on the results of the rollout.
# It prints the completed and failed tasks.
# 6. Return Value:
# The function returns the results of the inference process, which may include information about the tasks that were completed, failed, and any other relevant data.